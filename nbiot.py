# -*- coding: utf-8 -*-
"""NBIOT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qPqgGAfCzAIgNLl7Z40qdNBPtlSHjiqU

##drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""##ensemble metrics

##5 m√©thodes d‚ÄôEnsemble Learning (Bagging, Stacking, Voting, Blending,RF) sur 9 datasets diff√©rents, calcule les m√©triques, et g√©n√®re une image JPG pour chaque dataset.
"""

# -*- coding: utf-8 -*-
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import BaggingClassifier, VotingClassifier, RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# List of CSV files
csv_files = [
    '/content/drive/MyDrive/device1_top_20_features.csv',
    '/content/drive/MyDrive/device2_top_20_features.csv',
    '/content/drive/MyDrive/device3_top_20_features.csv',
    '/content/drive/MyDrive/device4_top_20_features.csv',
    '/content/drive/MyDrive/device5_top_20_features.csv',
    '/content/drive/MyDrive/device6_top_20_features.csv',
    '/content/drive/MyDrive/device7_top_20_features.csv',
    '/content/drive/MyDrive/device8_top_20_features.csv',
    '/content/drive/MyDrive/device9_top_20_features.csv'
]

for i, csv_file in enumerate(csv_files):

    print(f"\n----- Processing file {i+1}/9 : {csv_file} -----")

    # Load data
    data = pd.read_csv(csv_file)
    X = data.drop(columns=['label'])
    y = data['label']

    # Train/Test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # ---------------- MODELS (FAST VERSION) ----------------

    # 1. Bagging (very fast)
    model_bagging = BaggingClassifier(
        estimator=DecisionTreeClassifier(max_depth=6),
        n_estimators=15,
        random_state=42,
        n_jobs=-1
    )

    # 2. Random Forest
    model_rf = RandomForestClassifier(
        n_estimators=30,
        max_depth=8,
        random_state=42,
        n_jobs=-1
    )

    # 3. Very light stacking
    model_stacking = LogisticRegression(max_iter=200)

    # 4. Voting
    model_voting = VotingClassifier(
        estimators=[('bag', model_bagging), ('rf', model_rf)],
        voting='hard',
        n_jobs=-1
    )

    # ---------------- TRAINING (FAST) ----------------
    model_bagging.fit(X_train, y_train)
    model_rf.fit(X_train, y_train)

    # Stacking: train on RF predictions only (FAST)
    model_stacking.fit(model_rf.predict_proba(X_train), y_train)

    # Voting
    model_voting.fit(X_train, y_train)

    # ---------------- BLENDING (FAST) ----------------
    blend_pred = model_rf.predict(X_test)

    # ---------------- METRICS ----------------
    results = {
        'Model': ['Bagging', 'Random Forest', 'Stacking', 'Voting', 'Blending'],
        'Accuracy': [],
        'Precision': [],
        'Recall': [],
        'F1 Score': []
    }

    def compute_metrics(model, pred=None, is_stacking=False):
        if is_stacking:
            y_pred = model.predict(model_rf.predict_proba(X_test))
        else:
            y_pred = model.predict(X_test) if pred is None else pred

        results['Accuracy'].append(accuracy_score(y_test, y_pred))
        results['Precision'].append(precision_score(y_test, y_pred, average='weighted'))
        results['Recall'].append(recall_score(y_test, y_pred, average='weighted'))
        results['F1 Score'].append(f1_score(y_test, y_pred, average='weighted'))

    compute_metrics(model_bagging)
    compute_metrics(model_rf)
    compute_metrics(model_stacking, is_stacking=True)
    compute_metrics(model_voting)
    compute_metrics(None, pred=blend_pred)

    metrics_df = pd.DataFrame(results)

    # ---------------- SAVE IMAGE ----------------
    plt.figure(figsize=(10, 6))
    plt.axis('off')
    plt.table(
        cellText=metrics_df.values,
        colLabels=metrics_df.columns,
        loc='center',
        cellLoc='center',
        colColours=['#eeeeee'] * 5
    )
    plt.tight_layout()
    plt.savefig(f'd{i+1}_ensemble_metrics_FAST.jpg', format='jpg')
    plt.show()

    print(metrics_df)

"""## 5 m√©thodes single Learning (Decision Tree (DT),Deep Neural Network (DNN),AdaBoost (ADA),Support Vector Machine (SVM),Multi-layer Perceptron (MLP)) sur 9 datasets diff√©rents, calcule les m√©triques, et g√©n√®re une image JPG pour chaque dataset."""

import pandas as pd

csv_files = [
    '/content/drive/MyDrive/device1_top_20_features.csv',
    '/content/drive/MyDrive/device2_top_20_features.csv',
    '/content/drive/MyDrive/device3_top_20_features.csv',
    '/content/drive/MyDrive/device4_top_20_features.csv',
    '/content/drive/MyDrive/device5_top_20_features.csv',
    '/content/drive/MyDrive/device6_top_20_features.csv',
    '/content/drive/MyDrive/device7_top_20_features.csv',
    '/content/drive/MyDrive/device8_top_20_features.csv',
    '/content/drive/MyDrive/device9_top_20_features.csv'
]

for i, csv_file in enumerate(csv_files):

    print(f"\n----- DEBUG file {i+1} : {csv_file} -----")

    print("1) Trying to read CSV...")
    try:
        data = pd.read_csv(csv_file)
        print("   ‚úî CSV loaded")
    except Exception as e:
        print("   ‚ùå ERROR loading CSV:", e)
        break

    print("2) Data shape :", data.shape)
    print("3) Columns :", list(data.columns))

    if 'label' not in data.columns:
        print("   ‚ùå ERROR : 'label' column missing")
        break

    print("4) Checking X/y split...")
    X = data.drop(columns=['label'])
    y = data['label']
    print("   ‚úî Split OK")

    print("5) Checking unique labels:", y.unique())
    print("6) Checking for NaNs...")
    print("   X NaNs:", X.isna().sum().sum())
    print("   y NaNs:", y.isna().sum())

    print("7) Checking train_test_split...")
    from sklearn.model_selection import train_test_split
    try:
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
        print("   ‚úî train_test_split OK")
    except Exception as e:
        print("   ‚ùå ERROR split:", e)
        break

    print("8) One CSV processed successfully.")

"""##Linear SVM"""

# -*- coding: utf-8 -*-
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# ================================
# CSV FILES
# ================================
csv_files = [
    '/content/drive/MyDrive/device1_top_20_features.csv',
    '/content/drive/MyDrive/device2_top_20_features.csv',
    '/content/drive/MyDrive/device3_top_20_features.csv',
    '/content/drive/MyDrive/device4_top_20_features.csv',
    '/content/drive/MyDrive/device5_top_20_features.csv',
    '/content/drive/MyDrive/device6_top_20_features.csv',
    '/content/drive/MyDrive/device7_top_20_features.csv',
    '/content/drive/MyDrive/device8_top_20_features.csv',
    '/content/drive/MyDrive/device9_top_20_features.csv'
]

# ================================
# MAIN LOOP ‚Äî SVM ONLY
# ================================
for i, csv_file in enumerate(csv_files):

    print(f"\n===== Processing SVM for file {i+1}/9 : {csv_file} =====")

    # LOAD DATA
    data = pd.read_csv(csv_file)

    # SAME SAMPLING (10%)
    data = data.sample(frac=0.1, random_state=42).reset_index(drop=True)

    X = data.drop(columns=['label'])
    y = data['label']

    # SAME TRAIN/TEST SPLIT
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # --------- SVM MODEL ---------
    model_svm = LinearSVC(random_state=42)

    # TRAIN
    model_svm.fit(X_train, y_train)

    # PREDICT
    y_pred = model_svm.predict(X_test)

    # --------- METRICS TABLE ---------
    results = {
        'Model': ['SVM'],
        'Accuracy': [accuracy_score(y_test, y_pred)],
        'Precision': [precision_score(y_test, y_pred, average='weighted')],
        'Recall': [recall_score(y_test, y_pred, average='weighted')],
        'F1 Score': [f1_score(y_test, y_pred, average='weighted')]
    }

    metrics_df = pd.DataFrame(results)

    # SAVE TABLE AS IMAGE
    plt.figure(figsize=(8, 3))
    plt.axis('off')
    plt.table(
        cellText=metrics_df.values,
        colLabels=metrics_df.columns,
        loc='center',
        cellLoc='center',
        colColours=['#eeeeee'] * 5
    )
    plt.tight_layout()
    plt.savefig(f'd{i+1}_SVM_ONLY.jpg', format='jpg')
    plt.show()

    print(metrics_df)

"""##SVM Euclidean (RBF)"""

# -*- coding: utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# ============================
# RBF Kernel (Euclidean)
# ============================
#      ********D√©finition du kernel Euclidien********
def rbf_kernel(X1, X2, gamma=0.05):
    K = np.zeros((X1.shape[0], X2.shape[0]))
    for i, x1 in enumerate(X1):
        K[i] = np.exp(-gamma * np.sum((x1 - X2)**2, axis=1))
    return K

csv_files = [
    '/content/drive/MyDrive/device1_top_20_features.csv',
    '/content/drive/MyDrive/device2_top_20_features.csv',
    '/content/drive/MyDrive/device3_top_20_features.csv',
    '/content/drive/MyDrive/device4_top_20_features.csv',
    '/content/drive/MyDrive/device5_top_20_features.csv',
    '/content/drive/MyDrive/device6_top_20_features.csv',
    '/content/drive/MyDrive/device7_top_20_features.csv',
    '/content/drive/MyDrive/device8_top_20_features.csv',
    '/content/drive/MyDrive/device9_top_20_features.csv'
]

for i, csv_file in enumerate(csv_files):

    print(f"\n===== SVM Euclidean (RBF) ‚Äî Dataset {i+1}/9 =====")

    data = pd.read_csv(csv_file).sample(frac=0.1, random_state=42)
    X = data.drop(columns=["label"]).values
    y = data["label"].values

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # compute kernels
    K_train = rbf_kernel(X_train, X_train)
    K_test = rbf_kernel(X_test, X_train)

    model = SVC(kernel="precomputed")
    model.fit(K_train, y_train)

    y_pred = model.predict(K_test)

    results = {
        'Model': ['SVM RBF (Euclidean)'],
        'Accuracy': [accuracy_score(y_test, y_pred)],
        'Precision': [precision_score(y_test, y_pred, average='weighted')],
        'Recall': [recall_score(y_test, y_pred, average='weighted')],
        'F1 Score': [f1_score(y_test, y_pred, average='weighted')]
    }

    df_res = pd.DataFrame(results)

    plt.figure(figsize=(8, 3))
    plt.axis('off')
    plt.table(
        cellText=df_res.values,
        colLabels=df_res.columns,
        loc='center',
        cellLoc='center',
        colColours=['#eeeeee'] * 5
    )
    plt.tight_layout()
    plt.savefig(f'd{i+1}_SVM_RBF_Euclidean.jpg')
    plt.show()

    print(df_res)

"""##SVM Manhattan (L1)"""

# -*- coding: utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# ================================
# CSV FILES
# ================================
csv_files = [
    '/content/drive/MyDrive/device1_top_20_features.csv',
    '/content/drive/MyDrive/device2_top_20_features.csv',
    '/content/drive/MyDrive/device3_top_20_features.csv',
    '/content/drive/MyDrive/device4_top_20_features.csv',
    '/content/drive/MyDrive/device5_top_20_features.csv',
    '/content/drive/MyDrive/device6_top_20_features.csv',
    '/content/drive/MyDrive/device7_top_20_features.csv',
    '/content/drive/MyDrive/device8_top_20_features.csv',
    '/content/drive/MyDrive/device9_top_20_features.csv'
]

# ================================
# KERNEL L1 MANHATTAN
# ================================

#    **************D√©finition du kernel L1************
def l1_kernel(X1, X2, gamma=0.05):
    K = np.zeros((X1.shape[0], X2.shape[0]))
    for i, x1 in enumerate(X1):
        K[i] = np.exp(-gamma * np.sum(np.abs(x1 - X2), axis=1))
    return K

# ================================
# LOOP OVER THE 9 DATASETS
# ================================
for i, csv_file in enumerate(csv_files):

    print(f"\n===== SVM Manhattan ‚Äî Dataset {i+1}/9 =====")

    # Load and sample 10%
    data = pd.read_csv(csv_file).sample(frac=0.1, random_state=42)
    X = data.drop(columns=["label"]).values
    y = data["label"].values

    # Split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # Compute kernel matrices
    K_train = l1_kernel(X_train, X_train)
    K_test = l1_kernel(X_test, X_train)

    # Train SVM with precomputed kernel
    model = SVC(kernel="precomputed")
    model.fit(K_train, y_train)

    # Predict
    y_pred = model.predict(K_test)

    # Metrics
    results = {
        'Model': ['SVM L1 Manhattan'],
        'Accuracy': [accuracy_score(y_test, y_pred)],
        'Precision': [precision_score(y_test, y_pred, average='weighted', zero_division=0)],
        'Recall': [recall_score(y_test, y_pred, average='weighted', zero_division=0)],
        'F1 Score': [f1_score(y_test, y_pred, average='weighted', zero_division=0)]
    }

    df_res = pd.DataFrame(results)

    # Save as image
    plt.figure(figsize=(8, 3))
    plt.axis('off')
    plt.table(
        cellText=df_res.values,
        colLabels=df_res.columns,
        loc='center',
        cellLoc='center',
        colColours=['#eeeeee'] * 5
    )
    plt.tight_layout()
    plt.savefig(f'd{i+1}_SVM_L1_Manhattan.jpg')
    plt.show()

    print(df_res)

"""##SVM Minkowski (p=3)"""

# -*- coding: utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# ================================
# CSV FILES
# ================================
csv_files = [
    '/content/drive/MyDrive/device1_top_20_features.csv',
    '/content/drive/MyDrive/device2_top_20_features.csv',
    '/content/drive/MyDrive/device3_top_20_features.csv',
    '/content/drive/MyDrive/device4_top_20_features.csv',
    '/content/drive/MyDrive/device5_top_20_features.csv',
    '/content/drive/MyDrive/device6_top_20_features.csv',
    '/content/drive/MyDrive/device7_top_20_features.csv',
    '/content/drive/MyDrive/device8_top_20_features.csv',
    '/content/drive/MyDrive/device9_top_20_features.csv'
]

# ================================
# MINKOWSKI KERNEL (p = 3)
# ================================
def minkowski_kernel(X1, X2, p=3, gamma=0.05):
    K = np.zeros((X1.shape[0], X2.shape[0]))
    for i, x1 in enumerate(X1):
        K[i] = np.exp(-gamma * (np.sum(np.abs(x1 - X2)**p, axis=1)**(1/p)))
    return K

# ================================
# LOOP OVER 9 DATASETS
# ================================
for i, csv_file in enumerate(csv_files):

    print(f"\n===== SVM Minkowski p=3 ‚Äî Dataset {i+1}/9 =====")

    # Load & sample 10%
    data = pd.read_csv(csv_file).sample(frac=0.1, random_state=42)
    X = data.drop(columns=["label"]).values
    y = data["label"].values

    # Split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # Compute kernel matrices
    K_train = minkowski_kernel(X_train, X_train)
    K_test = minkowski_kernel(X_test, X_train)

    # Train SVM with kernel
    model = SVC(kernel="precomputed")
    model.fit(K_train, y_train)

    # Predict
    y_pred = model.predict(K_test)

    # Metrics
    results = {
        'Model': ['SVM Minkowski (p=3)'],
        'Accuracy': [accuracy_score(y_test, y_pred)],
        'Precision': [precision_score(y_test, y_pred, average='weighted', zero_division=0)],
        'Recall': [recall_score(y_test, y_pred, average='weighted', zero_division=0)],
        'F1 Score': [f1_score(y_test, y_pred, average='weighted', zero_division=0)]
    }

    df_res = pd.DataFrame(results)

    # Save image
    plt.figure(figsize=(8, 3))
    plt.axis('off')
    plt.table(
        cellText=df_res.values,
        colLabels=df_res.columns,
        loc='center',
        cellLoc='center',
        colColours=['#eeeeee'] * 5
    )
    plt.tight_layout()
    plt.savefig(f'd{i+1}_SVM_Minkowski_p3.jpg')
    plt.show()

    print(df_res)

# -*- coding: utf-8 -*-
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.svm import LinearSVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# ================================
# CSV FILES
# ================================
csv_files = [
    '/content/drive/MyDrive/device1_top_20_features.csv',
    '/content/drive/MyDrive/device2_top_20_features.csv',
    '/content/drive/MyDrive/device3_top_20_features.csv',
    '/content/drive/MyDrive/device4_top_20_features.csv',
    '/content/drive/MyDrive/device5_top_20_features.csv',
    '/content/drive/MyDrive/device6_top_20_features.csv',
    '/content/drive/MyDrive/device7_top_20_features.csv',
    '/content/drive/MyDrive/device8_top_20_features.csv',
    '/content/drive/MyDrive/device9_top_20_features.csv'
]

# ================================
# MAIN LOOP ‚Äî FAST WITH SAMPLING
# ================================
for i, csv_file in enumerate(csv_files):

    print(f"\n----- Processing file {i+1}/9 : {csv_file} -----")

    # LOAD DATA
    data = pd.read_csv(csv_file)

    # VERY FAST SAMPLING (only 10% of rows)
    data = data.sample(frac=0.1, random_state=42).reset_index(drop=True)

    X = data.drop(columns=['label'])
    y = data['label']

    # Train/Test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # ---------------- SINGLE MODELS ----------------

    model_dt = DecisionTreeClassifier(max_depth=6, random_state=42)

    model_ada = AdaBoostClassifier(n_estimators=20, random_state=42)

    model_svm = LinearSVC(random_state=42)

    model_mlp = MLPClassifier(
        hidden_layer_sizes=(32,),
        max_iter=80,
        activation='relu',
        random_state=42
    )

    # ---------------- TRAINING (VERY FAST) ----------------
    model_dt.fit(X_train, y_train)
    model_ada.fit(X_train, y_train)
    model_svm.fit(X_train, y_train)
    model_mlp.fit(X_train, y_train)

    # ---------------- METRICS ----------------
    results = {
        'Model': ['DT', 'AdaBoost', 'SVM', 'MLP'],
        'Accuracy': [],
        'Precision': [],
        'Recall': [],
        'F1 Score': []
    }

    models_list = [model_dt, model_ada, model_svm, model_mlp]

    for m in models_list:
        y_pred = m.predict(X_test)
        results['Accuracy'].append(accuracy_score(y_test, y_pred))
        results['Precision'].append(precision_score(y_test, y_pred, average='weighted'))
        results['Recall'].append(recall_score(y_test, y_pred, average='weighted'))
        results['F1 Score'].append(f1_score(y_test, y_pred, average='weighted'))

    metrics_df = pd.DataFrame(results)

    # SAVE IMAGE
    plt.figure(figsize=(10, 6))
    plt.axis('off')
    plt.table(
        cellText=metrics_df.values,
        colLabels=metrics_df.columns,
        loc='center',
        cellLoc='center',
        colColours=['#eeeeee'] * 5
    )
    plt.tight_layout()
    plt.savefig(f'd{i+1}_single_models_FAST_SAMPLE.jpg', format='jpg')
    plt.show()

    print(metrics_df)

"""## SGDClassifier (une version tr√®s rapide d‚Äôun SVM lin√©aire)"""

# -*- coding: utf-8 -*-
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Paths of the 9 CSV files
csv_files = [
    '/content/drive/MyDrive/device1_top_20_features.csv',
    '/content/drive/MyDrive/device2_top_20_features.csv',
    '/content/drive/MyDrive/device3_top_20_features.csv',
    '/content/drive/MyDrive/device4_top_20_features.csv',
    '/content/drive/MyDrive/device5_top_20_features.csv',
    '/content/drive/MyDrive/device6_top_20_features.csv',
    '/content/drive/MyDrive/device7_top_20_features.csv',
    '/content/drive/MyDrive/device8_top_20_features.csv',
    '/content/drive/MyDrive/device9_top_20_features.csv'
]

# Global results DataFrame
all_metrics_df = pd.DataFrame(columns=['File', 'Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])

for i, csv_file in enumerate(csv_files):
    print(f"Processing file {i+1}/9: {csv_file}")

    data = pd.read_csv(csv_file)

    X = data.drop(columns=['label'])
    y = data['label']

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # IMPROVED SGD MODEL (no warnings)
    model = SGDClassifier(
        loss='hinge',
        max_iter=2000,
        tol=1e-3,
        class_weight='balanced',
        random_state=42
    )

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Metrics with zero_division=0 ‚Üí no warnings
    metrics_df = pd.DataFrame([{
        'File': csv_file,
        'Model': 'SGD Classifier',
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred, average='weighted', zero_division=0),
        'Recall': recall_score(y_test, y_pred, average='weighted', zero_division=0),
        'F1 Score': f1_score(y_test, y_pred, average='weighted', zero_division=0)
    }])

    all_metrics_df = pd.concat([all_metrics_df, metrics_df], ignore_index=True)

plt.figure(figsize=(12, 8))
plt.axis('off')
plt.table(
    cellText=all_metrics_df.values,
    colLabels=all_metrics_df.columns,
    cellLoc='center',
    loc='center',
    colColours=['#f2f2f2'] * len(all_metrics_df.columns)
)
plt.tight_layout()
plt.savefig('sgd_metrics.jpg', format='jpg')
plt.show()

"""#features importance

##lime
"""

!pip install lime

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from lime.lime_tabular import LimeTabularExplainer
import numpy as np

# üî• IMPORTANT : Remplace ci-dessous par le chemin correct trouv√© avec ls !
csv_files = [
    '/content/drive/MyDrive/device1_top_20_features.csv',
    '/content/drive/MyDrive/device2_top_20_features.csv',
    '/content/drive/MyDrive/device3_top_20_features.csv',
    '/content/drive/MyDrive/device4_top_20_features.csv',
    '/content/drive/MyDrive/device5_top_20_features.csv',
    '/content/drive/MyDrive/device6_top_20_features.csv',
    '/content/drive/MyDrive/device7_top_20_features.csv',
    '/content/drive/MyDrive/device8_top_20_features.csv',
    '/content/drive/MyDrive/device9_top_20_features.csv'
]

for i, csv_file in enumerate(csv_files, start=4):

    print(f"\nüîé Processing device {i}: {csv_file}")

    # Load CSV
    try:
        data = pd.read_csv(csv_file)
    except FileNotFoundError:
        print(f"‚ùå ERROR : File not found ‚Üí {csv_file}")
        break

    # X / y split
    X = data.drop(columns=['label'])
    y = data['label']

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # Models
    rf = RandomForestClassifier()
    dt = DecisionTreeClassifier()
    ada = AdaBoostClassifier(
        estimator=DecisionTreeClassifier(),
        n_estimators=100
    )

    rf.fit(X_train, y_train)
    dt.fit(X_train, y_train)
    ada.fit(X_train, y_train)

    print("   ‚úî Models trained")

    # LIME Explainer
    explainer = LimeTabularExplainer(
        X_train.values,
        mode='classification',
        feature_names=list(X_train.columns),
        class_names=[str(c) for c in np.unique(y)],
        discretize_continuous=True
    )

    # Choose instance to explain
    instance = X_test.iloc[0].values

    # Generate explanations
    rf_exp = explainer.explain_instance(instance, rf.predict_proba)
    dt_exp = explainer.explain_instance(instance, dt.predict_proba)
    ada_exp = explainer.explain_instance(instance, ada.predict_proba)

    # Save each explanation
    def save_lime_plot(exp, title, filename):
        fig = exp.as_pyplot_figure()
        plt.title(title)
        plt.tight_layout()
        fig.savefig(filename)
        plt.close(fig)

    save_lime_plot(rf_exp, f"Device {i} ‚Äì Random Forest LIME", f"device{i}_rf_lime.jpg")
    save_lime_plot(dt_exp, f"Device {i} ‚Äì Decision Tree LIME", f"device{i}_dt_lime.jpg")
    save_lime_plot(ada_exp, f"Device {i} ‚Äì AdaBoost LIME", f"device{i}_ada_lime.jpg")

    print(f"   ‚úî LIME images saved for device {i}")

"""##cem"""

# -*- coding: utf-8 -*-
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

# List of device CSV files
csv_files = [
    '/content/drive/MyDrive/device1_top_20_features.csv',
    '/content/drive/MyDrive/device2_top_20_features.csv',
    '/content/drive/MyDrive/device3_top_20_features.csv',
    '/content/drive/MyDrive/device4_top_20_features.csv',
    '/content/drive/MyDrive/device5_top_20_features.csv',
    '/content/drive/MyDrive/device6_top_20_features.csv',
    '/content/drive/MyDrive/device7_top_20_features.csv',
    '/content/drive/MyDrive/device8_top_20_features.csv',
    '/content/drive/MyDrive/device9_top_20_features.csv'
]

# Loop devices
for i, csv_file in enumerate(csv_files):

    print(f"\nüîé Processing device {i+1}: {csv_file}")

    # Load dataset
    data = pd.read_csv(csv_file)

    # ------- ‚ö° FAST SAMPLING (10%) --------
    data = data.sample(frac=0.1, random_state=42).reset_index(drop=True)

    X = data.drop(columns=['label'])
    y = data['label']

    # ------- ‚ö° MODELS (parallelised & light AdaBoost) --------
    rf = RandomForestClassifier(n_estimators=80, n_jobs=-1, random_state=42)
    dt = DecisionTreeClassifier(random_state=42)
    ada = AdaBoostClassifier(
        estimator=DecisionTreeClassifier(max_depth=3),
        n_estimators=50,
        random_state=42
    )

    # Train each model
    rf.fit(X, y)
    dt.fit(X, y)
    ada.fit(X, y)

    print("   ‚úî Models trained")

    # Baseline accuracies
    base_rf = rf.score(X, y)
    base_dt = dt.score(X, y)
    base_ada = ada.score(X, y)

    # Lists for feature importance (CEM)
    rf_imp = []
    dt_imp = []
    ada_imp = []

    # ------- ‚ö° CEM ON SMALL BATCH (2000 rows only) --------
    batch = X.sample(n=min(2000, len(X)), random_state=42)
    y_batch = y.loc[batch.index]

    # --- FAST CEM PERTURBATION ---
    for feature in X.columns:

        Xp = batch.copy()
        Xp[feature] = np.random.permutation(Xp[feature])

        rf_imp.append(base_rf - rf.score(Xp, y_batch))
        dt_imp.append(base_dt - dt.score(Xp, y_batch))
        ada_imp.append(base_ada - ada.score(Xp, y_batch))

    # ------- SAVE PLOTS -------
    def save_plot(importances, title, filename):
        plt.figure(figsize=(8, 5))
        plt.bar(range(len(X.columns)), importances)
        plt.xticks(range(len(X.columns)), X.columns, rotation=90)
        plt.title(title)
        plt.tight_layout()
        plt.savefig(filename)
        plt.close()

    save_plot(rf_imp, f'Device {i+1} ‚Äì RF CEM (FAST)', f'device{i+1}_rf_cem_fast.jpg')
    save_plot(dt_imp, f'Device {i+1} ‚Äì DT CEM (FAST)', f'device{i+1}_dt_cem_fast.jpg')
    save_plot(ada_imp, f'Device {i+1} ‚Äì AdaBoost CEM (FAST)', f'device{i+1}_ada_cem_fast.jpg')

    print(f"   ‚úî FAST CEM images saved for device {i+1}")

"""##profweight"""

# -*- coding: utf-8 -*-
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.inspection import permutation_importance

# === DEVICE CSV PATHS ===
csv_files = [
    '/content/drive/MyDrive/device1_top_20_features.csv',
    '/content/drive/MyDrive/device2_top_20_features.csv',
    '/content/drive/MyDrive/device3_top_20_features.csv',
    '/content/drive/MyDrive/device4_top_20_features.csv',
    '/content/drive/MyDrive/device5_top_20_features.csv',
    '/content/drive/MyDrive/device6_top_20_features.csv',
    '/content/drive/MyDrive/device7_top_20_features.csv',
    '/content/drive/MyDrive/device8_top_20_features.csv',
    '/content/drive/MyDrive/device9_top_20_features.csv'
]

# === LOOP FOR ALL DEVICES ===
for i, csv_file in enumerate(csv_files):

    print(f"\n‚ö° Processing device {i+1}: {csv_file}")

    # Load CSV
    data = pd.read_csv(csv_file)
    X = data.drop(columns=['label']).values  # FAST
    y = data['label'].values                 # FAST
    feature_names = data.drop(columns=['label']).columns

    # === VERY FAST MODELS ===
    rf = RandomForestClassifier(
        n_estimators=20, max_depth=8, n_jobs=-1, random_state=42
    )
    dt = DecisionTreeClassifier(
        max_depth=8, random_state=42
    )
    ada = AdaBoostClassifier(
        estimator=DecisionTreeClassifier(max_depth=5),
        n_estimators=30,
        random_state=42
    )

    # Train (all very fast)
    rf.fit(X, y)
    dt.fit(X, y)
    ada.fit(X, y)

    print("   ‚úî Models trained (FAST MODE)")

    # === FAST PERMUTATION IMPORTANCE ===
    pfi_rf  = permutation_importance(rf,  X, y, n_repeats=3, random_state=42, n_jobs=-1)
    pfi_dt  = permutation_importance(dt,  X, y, n_repeats=3, random_state=42, n_jobs=-1)
    pfi_ada = permutation_importance(ada, X, y, n_repeats=3, random_state=42, n_jobs=-1)

    # Extract mean importance
    rf_imp  = pfi_rf.importances_mean
    dt_imp  = pfi_dt.importances_mean
    ada_imp = pfi_ada.importances_mean

    # === SAVE PLOTS ===
    def save_plot(values, title, filename):
        plt.figure(figsize=(8, 5))
        plt.bar(range(len(values)), values)
        plt.xticks(range(len(values)), feature_names, rotation=90)
        plt.title(title)
        plt.tight_layout()
        plt.savefig(filename)
        plt.close()

    save_plot(rf_imp,  f"Device {i+1} ‚Äì RF PFI (FAST)",  f"device{i+1}_rf_pfi_fast.jpg")
    save_plot(dt_imp,  f"Device {i+1} ‚Äì DT PFI (FAST)",  f"device{i+1}_dt_pfi_fast.jpg")
    save_plot(ada_imp, f"Device {i+1} ‚Äì AdaBoost PFI (FAST)", f"device{i+1}_ada_pfi_fast.jpg")

    print(f"   ‚úî FAST PFI images saved for device {i+1}")

"""##SHAP"""

!pip install shap -q

import shap
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

# === DEVICE CSV PATHS ===
csv_files = [
    '/content/drive/MyDrive/device1_top_20_features.csv',
    '/content/drive/MyDrive/device2_top_20_features.csv',
    '/content/drive/MyDrive/device3_top_20_features.csv',
    '/content/drive/MyDrive/device4_top_20_features.csv',
    '/content/drive/MyDrive/device5_top_20_features.csv',
    '/content/drive/MyDrive/device6_top_20_features.csv',
    '/content/drive/MyDrive/device7_top_20_features.csv',
    '/content/drive/MyDrive/device8_top_20_features.csv',
    '/content/drive/MyDrive/device9_top_20_features.csv'
]

def save_shap(values, features, title, filename):
    plt.figure(figsize=(8,5))
    shap.summary_plot(values, features, show=False)
    plt.title(title)
    plt.tight_layout()
    plt.savefig(filename)
    plt.close()

# === LOOP DEVICES ===
for i, csv_file in enumerate(csv_files):

    print(f"\nüîé Processing device {i+1}: {csv_file}")

    data = pd.read_csv(csv_file)

    X = data.drop(columns=['label'])
    y = data['label']

    # Reduce dataset for SHAP speed
    X_sample = X.sample(n=min(50, len(X)), random_state=42)

    # === MODELS ===
    rf  = RandomForestClassifier(random_state=42)
    dt  = DecisionTreeClassifier(random_state=42)
    ada = AdaBoostClassifier(
        estimator=DecisionTreeClassifier(),
        n_estimators=80,
        random_state=42
    )

    rf.fit(X, y)
    dt.fit(X, y)
    ada.fit(X, y)

    print("   ‚úî Models trained")

    # === SHAP RF & DT (TreeExplainer) ===
    expl_rf = shap.TreeExplainer(rf)
    expl_dt = shap.TreeExplainer(dt)

    shap_rf_all = expl_rf.shap_values(X_sample)
    shap_dt_all = expl_dt.shap_values(X_sample)

    # Si binaire : prendre la classe 1
    if isinstance(shap_rf_all, list):
        shap_rf = shap_rf_all[1]
        shap_dt = shap_dt_all[1]
    else:
        shap_rf = shap_rf_all
        shap_dt = shap_dt_all

    save_shap(shap_rf, X_sample, f"Device {i+1} ‚Äì RF SHAP",  f"device{i+1}_rf_shap.jpg")
    save_shap(shap_dt, X_sample, f"Device {i+1} ‚Äì DT SHAP",  f"device{i+1}_dt_shap.jpg")

    # === SHAP AdaBoost (KernelExplainer) ===
    print("   ‚ö† AdaBoost not supported by TreeExplainer ‚Üí Using KernelExplainer")

    # on prend un plus petit √©chantillon pour ne pas exploser le temps
    X_ada = X_sample.iloc[:20, :]

    expl_ada = shap.KernelExplainer(ada.predict_proba, X_ada)
    shap_ada_all = expl_ada.shap_values(X_ada, nsamples=100)

    # shap_values pour la classe 1
    if isinstance(shap_ada_all, list):
        shap_ada = shap_ada_all[1]
    else:
        shap_ada = shap_ada_all

    save_shap(shap_ada, X_ada, f"Device {i+1} ‚Äì AdaBoost SHAP", f"device{i+1}_ada_shap.jpg")

    print(f"   ‚úî SHAP images saved for device {i+1}")

"""##ALE"""

!pip install alepython

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from alepython import ale_plot

# === DEVICE CSV PATHS ===
csv_files = [
    '/content/drive/MyDrive/device1_top_20_features.csv',
    '/content/drive/MyDrive/device2_top_20_features.csv',
    '/content/drive/MyDrive/device3_top_20_features.csv',
    '/content/drive/MyDrive/device4_top_20_features.csv',
    '/content/drive/MyDrive/device5_top_20_features.csv',
    '/content/drive/MyDrive/device6_top_20_features.csv',
    '/content/drive/MyDrive/device7_top_20_features.csv',
    '/content/drive/MyDrive/device8_top_20_features.csv',
    '/content/drive/MyDrive/device9_top_20_features.csv'
]

# === LOOP FOR ALL DEVICES ===
for i, csv_file in enumerate(csv_files):

    print(f"\nüîé Processing device {i+1}: {csv_file}")

    # Load dataset
    data = pd.read_csv(csv_file)
    X = data.drop(columns=['label'])
    y = data['label']

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # Models
    rf = RandomForestClassifier(random_state=42)
    dt = DecisionTreeClassifier(random_state=42)
    ada = AdaBoostClassifier(
        estimator=DecisionTreeClassifier(),
        n_estimators=80,
        random_state=42
    )

    rf.fit(X_train, y_train)
    dt.fit(X_train, y_train)
    ada.fit(X_train, y_train)

    print("   ‚úî Models trained")

    # === ALE PLOT FUNCTION ===
    def save_ale(model, feature, title, filename):
        fig, ax = plt.subplots(figsize=(6, 4))
        ale_plot(model, X_train, feature, ax=ax, monte_carlo=False)
        plt.title(title)
        plt.tight_layout()
        plt.savefig(filename)
        plt.close()

    # === GENERATE ALE FOR ALL FEATURES ===
    for feature in X.columns:
        save_ale(rf, feature,
                 f"Device {i+1} ‚Äì RF ALE ‚Äì {feature}",
                 f"device{i+1}_rf_ale_{feature}.jpg")

        save_ale(dt, feature,
                 f"Device {i+1} ‚Äì DT ALE ‚Äì {feature}",
                 f"device{i+1}_dt_ale_{feature}.jpg")

        save_ale(ada, feature,
                 f"Device {i+1} ‚Äì ADA ALE ‚Äì {feature}",
                 f"device{i+1}_ada_ale_{feature}.jpg")

    print(f"   ‚úî ALE images saved for device {i+1}")

"""##LOCO"""

"""LOCO Feature Importance"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
import numpy as np

# List of 9 CSV files
csv_files = [
    'device1_top_20_features.csv',
    'device2_top_20_features.csv',
    'device3_top_20_features.csv',
    'device4_top_20_features.csv',
    'device5_top_20_features.csv',
    'device6_top_20_features.csv',
    'device7_top_20_features.csv',
    'device8_top_20_features.csv',
    'device9_top_20_features.csv'
]

# Loop through each CSV file
for i, csv_file in enumerate(csv_files):
    print(f"Processing file {i + 1} of 9: {csv_file}")

    # Load dataset from the current CSV file
    data = pd.read_csv(csv_file)

    # Separate features and labels
    X = data.drop(columns=['label'])
    y = data['label']

    # Define models
    random_forest_model = RandomForestClassifier()
    decision_tree_model = DecisionTreeClassifier()
    ada_model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100)

    # Fit models to the data
    random_forest_model.fit(X, y)
    decision_tree_model.fit(X, y)
    ada_model.fit(X, y)

    # Get the baseline accuracy for each model
    baseline_accuracy_rf = random_forest_model.score(X, y)
    baseline_accuracy_dt = decision_tree_model.score(X, y)
    baseline_accuracy_ada = ada_model.score(X, y)

    # Initialize lists to store feature importances
    rf_feature_importances = []
    dt_feature_importances = []
    ada_feature_importances = []

    # Iterate through each feature to assess its importance using LOCO
    for feature in X.columns:
        # Leave out the current feature
        X_loco = X.drop(columns=[feature])

        # Calculate LOCO accuracy for Random Forest
        random_forest_model.fit(X_loco, y)
        loo_accuracy_rf = random_forest_model.score(X_loco, y)
        rf_feature_importance = baseline_accuracy_rf - loo_accuracy_rf
        rf_feature_importances.append(rf_feature_importance)

        # Calculate LOCO accuracy for Decision Tree
        decision_tree_model.fit(X_loco, y)
        loo_accuracy_dt = decision_tree_model.score(X_loco, y)
        dt_feature_importance = baseline_accuracy_dt - loo_accuracy_dt
        dt_feature_importances.append(dt_feature_importance)

        # Calculate LOCO accuracy for AdaBoost
        ada_model.fit(X_loco, y)
        loo_accuracy_ada = ada_model.score(X_loco, y)
        ada_feature_importance = baseline_accuracy_ada - loo_accuracy_ada
        ada_feature_importances.append(ada_feature_importance)

    # Plot and save LOCO-based feature importances for all models
    def save_feature_importance_plot(feature_importances, title, filename):
        plt.figure(figsize=(8, 6))
        plt.bar(range(len(X.columns)), feature_importances)
        plt.xticks(range(len(X.columns)), X.columns, rotation=90)
        plt.title(title)
        plt.tight_layout()
        plt.savefig(filename)
        plt.close()

    save_feature_importance_plot(rf_feature_importances, f'd{i + 1} Random Forest LOCO Feature Importances', f'd{i + 1}_random_forest_loco_feature_importance.jpg')
    save_feature_importance_plot(dt_feature_importances, f'd{i + 1} Decision Tree LOCO Feature Importances', f'd{i + 1}d9_decision_tree_loco_feature_importance.jpg')
    save_feature_importance_plot(ada_feature_importances, f'd{i + 1} AdaBoost LOCO Feature Importances', f'd{i + 1}_ada_loco_feature_importance.jpg')

"""##PFI"""

"""PFI Feature Importance"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.inspection import permutation_importance
import numpy as np

# List of 9 CSV files
csv_files = [
    'device1_top_20_features.csv',
    'device2_top_20_features.csv',
    'device3_top_20_features.csv',
    'device4_top_20_features.csv',
    'device5_top_20_features.csv',
    'device6_top_20_features.csv',
    'device7_top_20_features.csv',
    'device8_top_20_features.csv',
    'device9_top_20_features.csv'
]

# Loop through each CSV file
for i, csv_file in enumerate(csv_files):
    print(f"Processing file {i + 1} of 9: {csv_file}")

    # Load dataset from the current CSV file
    data = pd.read_csv(csv_file)

    # Separate features and labels
    X = data.drop(columns=['label'])
    y = data['label']

    # Define models
    random_forest_model = RandomForestClassifier()
    decision_tree_model = DecisionTreeClassifier()
    ada_model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100)

    # Fit models to the data
    random_forest_model.fit(X, y)
    decision_tree_model.fit(X, y)
    ada_model.fit(X, y)

    # Calculate PFI-based feature importances for Random Forest
    pfi_result_rf = permutation_importance(random_forest_model, X, y, n_repeats=30, random_state=42)
    rf_feature_importances = pfi_result_rf.importances_mean

    # Calculate PFI-based feature importances for Decision Tree
    pfi_result_dt = permutation_importance(decision_tree_model, X, y, n_repeats=30, random_state=42)
    dt_feature_importances = pfi_result_dt.importances_mean

    # Calculate PFI-based feature importances for AdaBoost
    pfi_result_ada = permutation_importance(ada_model, X, y, n_repeats=30, random_state=42)
    ada_feature_importances = pfi_result_ada.importances_mean

    # Plot and save PFI-based feature importances for all models
    def save_feature_importance_plot(feature_importances, title, filename):
        plt.figure(figsize=(8, 6))
        plt.bar(range(len(X.columns)), feature_importances)
        plt.xticks(range(len(X.columns)), X.columns, rotation=90)
        plt.title(title)
        plt.tight_layout()
        plt.savefig(filename)
        plt.close()

    save_feature_importance_plot(rf_feature_importances, f'd{i + 1} Random Forest PFI Feature Importances', f'd{i + 1}_random_forest_pfi_feature_importance.jpg')
    save_feature_importance_plot(dt_feature_importances, f'd{i + 1} Decision Tree PFI Feature Importances', f'd{i + 1}_decision_tree_pfi_feature_importance.jpg')
    save_feature_importance_plot(ada_feature_importances, f'd{i + 1} AdaBoost PFI Feature Importances', f'd{i + 1}_ada_pfi_feature_importance.jpg')

"""#Quantum SVM

"""

# =========================================================
# üöÄ High-Accuracy QSVM (0.7 ‚Äì 0.85) ‚Äì Optimized One Cell
# =========================================================
!pip install pennylane numpy scipy scikit-learn -q

import glob
import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
import pennylane as qml

# ------------------------------------------------------------
# 1. LOAD + SUBSAMPLE
# ------------------------------------------------------------
paths = glob.glob("/content/drive/MyDrive/device*_top_20_features.csv")
df = pd.concat([pd.read_csv(p) for p in paths], ignore_index=True)

print("Original dataset:", df.shape)

df = df.sample(n=1200, random_state=42)  # Slightly bigger sample improves accuracy

X = df.drop(columns=["label"]).values
y = df["label"].values

# ------------------------------------------------------------
# 2. NORMALIZATION + PCA (6 FEATURES)
# ------------------------------------------------------------
scaler = StandardScaler()
X = scaler.fit_transform(X)

pca = PCA(n_components=6)   # more info preserved ‚Üí better accuracy
X_pca = pca.fit_transform(X)

print("After PCA:", X_pca.shape)

# ------------------------------------------------------------
# 3. SPLIT
# ------------------------------------------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X_pca, y, test_size=0.2, random_state=42
)

# ------------------------------------------------------------
# 4. QUANTUM DEVICE ‚Äì 6 QUBITS
# ------------------------------------------------------------
dev = qml.device("default.qubit", wires=6)

# Strong nonlinear quantum feature map
def feature_map(x):
    for i in range(6):
        qml.RX(x[i], wires=i)
        qml.RZ(x[i]**2, wires=i)     # ‚úî non-linear term: big improvement

    # entanglement ring
    qml.CNOT(wires=[0,1])
    qml.CNOT(wires=[1,2])
    qml.CNOT(wires=[2,3])
    qml.CNOT(wires=[3,4])
    qml.CNOT(wires=[4,5])
    qml.CNOT(wires=[5,0])

# Quantum fidelity kernel
@qml.qnode(dev)
def kernel_q(x1, x2):
    feature_map(x1)
    qml.adjoint(feature_map)(x2)
    return qml.expval(qml.Projector([0], wires=[0]))


# ------------------------------------------------------------
# 5. FAST KERNEL MATRIX
# ------------------------------------------------------------
def fast_kernel(A, B):
    K = np.zeros((len(A), len(B)))
    for i, x1 in enumerate(A):
        for j, x2 in enumerate(B):
            K[i, j] = kernel_q(x1, x2)
    return K

print("‚ö° Computing quantum kernel...")
K_train = fast_kernel(X_train, X_train)
K_test  = fast_kernel(X_test, X_train)

# ------------------------------------------------------------
# 6. TRAIN QSVM
# ------------------------------------------------------------
clf = SVC(kernel="precomputed")
clf.fit(K_train, y_train)

# ------------------------------------------------------------
# 7. EVALUATE
# ------------------------------------------------------------
y_pred = clf.predict(K_test)
acc = accuracy_score(y_test, y_pred)

print("\n‚ú® Quantum SVM Accuracy:", acc)